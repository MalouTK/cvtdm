---
title: "Project"
subtitle: "Creating Value Through Data Mining"
author: "Malou-Tinette Kouango & Veronica Rowe"
date: "`r Sys.Date()`"
output: html_document
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Stroke Prediction Dataset 
# Clearing the console
```{r, warning = F,message = F, echo = TRUE}
#clear all the console
list=ls()
rm(list=ls()) 
cat("\014")
ls()
```

#Reading the Stroke dataset
```{r}
Stroke <- read.csv(file.choose())
```

# Loading the relevant packages
```{r, message =FALSE, warning=FALSE}
library(ggplot2) 
library(FNN)
library(dummies)
library(e1071)
library (caret)
library(reshape2)
library(dplyr)
library(class)
library(naniar)
require(ellipse)
```

 
### Exposing the data structure
```{r}
# Types of variables:
str(Stroke)
```

### We have 12 variables in the Stroke dataset, each having different types. Our outcome variable "Stroke" is a binary outcome, taking value 1 if the person had a stroke and 0 if they haven't had a stroke. => change it as factor
### Bmi is seen as a character because of the N/A => we must change N/A strings to real NA in R, and set the variable as numeric to be able to visualize it in our exploratory graphs.
### The variables gender, ever_married, work_type, Residence_type and smoking status are seen as character, when they are categorical => we must change them to factor for better data visualization.
### The variable id (= id of the patient in the hospital) is irrelevant to our analysis => we remove it

```{r}
# Changing N/A characters in bmi to real NA:
Stroke = replace_with_na(Stroke, replace = list(bmi = "N/A")) # bmi has NA now
# Change bmi from character to numerical variable:
Stroke$bmi <- as.numeric(Stroke$bmi)
Stroke$stroke <- as.factor(Stroke$stroke)

# Change character variables as categorical (factor)
Stroke[,c(2,6:8,11)] <- lapply(Stroke[,c(2,6:8,11)], as.factor)

# Remove id row
Stroke = Stroke[,-1]
str(Stroke)
```


```{r}
# Show how many different values there is for each variable
sapply(Stroke, function(x) length(unique(x)))
summary(Stroke)
```
#### Using sapply function from TP1 to find out more about the variables. We realise that in gender, there is only one observation as categorised as "Other". 
### To simplify our analysis, we will later disregard the row with that observation, as it not significant enough to be taken into consideration (1/5110 = 0.01956947%) (row/observation number id = 56'156/ row = 3117). We will also disregard the ID column as it only represent the patient id in the hospital and thus has no impact on our outcome variable of interest Stroke.

## Finding out information on our outcome variable
```{r}
table(Stroke$stroke)
```
#### We see that 249 people had a stroke and 4861 did not. We have an unbalanced binary outcome. => we must put results in context


## Visualising numerical variables. 
```{r}
# Visualize the data:
# gender:
ggplot(data = Stroke, aes(x = gender, fill = stroke)) + geom_bar() + ggtitle("Gender vs Stroke")
# age:
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_boxplot() + ggtitle("Age vs Stroke")
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_histogram() + ggtitle("Age vs Stroke")
# hypertension:
ggplot(data = Stroke, aes(x = hypertension, fill = stroke)) + geom_bar() + ggtitle("Hypertension vs Stroke")
# heart disease:
ggplot(data = Stroke, aes(x = heart_disease, fill = stroke)) + geom_bar() + ggtitle("Heart disease vs Stroke")
# ever married:
ggplot(data = Stroke, aes(x = ever_married, fill = stroke)) + geom_bar() + ggtitle("Ever married vs Stroke")
# work type:
ggplot(data = Stroke, aes(x = work_type, fill = stroke)) + geom_bar() + ggtitle("Work type vs Stroke")
# Residence type:
ggplot(data = Stroke, aes(x = Residence_type, fill = stroke)) + geom_bar() + ggtitle("Residence type vs Stroke")
# Average glucose level:
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_boxplot() + ggtitle("Average glucose level vs Stroke")
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_histogram() + ggtitle("Average glucose level vs Stroke")
# BMI:
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_boxplot() + ggtitle("BMI vs Stroke")
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_histogram() + ggtitle("BMI vs Stroke")
# Smoking status:
ggplot(data = Stroke, aes(x = smoking_status, fill = stroke)) + geom_bar() + ggtitle("Smoking status vs Stroke")
```


```{r, warning = FALSE}
quantitative_data <- (Stroke[,c(3,9)])
quantitative_data_melt <- melt(quantitative_data)
quantitative_data_means <-  quantitative_data_melt %>% group_by(variable) %>% summarise(mean.x = mean(value))
quantitative_data_median<- quantitative_data_melt %>% group_by(variable) %>% summarise(median.x = median(value))

quantitative_data_hist <- ggplot(data = quantitative_data_melt, aes(value)) + geom_histogram(fill="transparent",color = "black") + facet_wrap(~variable, scale="free", ncol = 4) + geom_vline(aes(xintercept = mean.x),quantitative_data_means, color = "blue") +  geom_vline(aes(xintercept = median.x),quantitative_data_median, color = "red")  + theme_classic() 

quantitative_data_hist
```
#### In the histograms, we see that the average age of the patient is in his 40ties and the average glucose is right-skewed.


#### Boxplot visualisation
```{r}
par(mfrow = c(2, 3))
boxplot(id~hypertension,data = Stroke, main = "Hypertension distribution")
boxplot(id~heart_disease,data = Stroke,  main = "Heart disease distribution")
boxplot(id~ever_married,data = Stroke,  main = "Marriage status distribution")
boxplot(id~work_type,data = Stroke,  main = "Work Type distribution")
boxplot(id~Residence_type,data = Stroke,  main = "Residence type distribution")
boxplot(id~smoking_status,data = Stroke,  main = "Smoking status distribution")
boxplot(id~stroke,data = Stroke,  main = "Stroke distribution")
```
#### We see that most variables are evenly distributed, roughly same weights ("50% of the sample have the account and the other 50% have not, for the variables separated into 2 categories"). We see that there is a slight majority of people not accepting the Personal Loan (mean of 0 is higher than the one of 1). I am skeptical of these results as above we saw that less than 10% took the Loan(PL is 1), however in the boxplot it seems like thez are nearlz on the same level.

#### Preparing for analysis : 

````{r}
# Remove only row with gender = "Other"
Stroke = Stroke[-3117,]
```


#### Transforming the categorical variables with more than 2 categories into dummy variables (Here it concerns variables work_type (6) & smoking_status (10))

```{r}
set.seed(1)
Stroke2 = Stroke
# Put work_type as dummy:
work_type_Dummies <- cbind( Stroke[1:5], dummy(Stroke$work_type, sep = "_"), Stroke[7:11])
names(work_type_Dummies)[6:10] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
work_type_Dummies
Stroke2 <- work_type_Dummies
```

```{r}
# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke2[1:13], dummy(Stroke2$smoking_status, sep = "_"), Stroke2[15])
names(smoking_status_Dummies)[14:17] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
smoking_status_Dummies
Stroke2 <- smoking_status_Dummies
```


#### Partitioning the 2 groups (60% in Training, 40% in Valid) & normalizing the data 
```{r}
set.seed(1)
Train <- sample(row.names(Stroke), 0.6*dim(Stroke)[1])
Valid <- setdiff(row.names(Stroke), Train)
Train <- Stroke[Train, ]
Valid <- Stroke[Valid, ]
dim(Train)
dim(Valid)
# The data has been correctly partitioned into 60% (3'065 obs) in T and 40% in V (2'044 obs)
```


#### Initializing training, validation data, using preProcess()
```{r}
# initialize normalized training and validation sets:
Train_norm <- Train
Valid_norm <- Valid
Stroke_normalized <- Stroke

# use preProcess() from the caret package to normalize the variables age, average_glucose and bmi:
Value.norm <- preProcess(Train[, c(2,8,9)], method= "range")

Train_norm[, c(2,8,9)] <- predict(Value.norm, Train[, c(2,8,9)])
Valid_norm[, c(2,8,9)] <- predict(Value.norm, Valid[, c(2,8,9)])
Stroke_normalized[, c(2,8,9)] <- predict(Value.norm, Stroke[, c(2,8,9)])
```

```{r}
# 1) Logistic Regression
logit.reg <- glm(stroke ~., data = Train, family = "binomial")
options(scipen = 999)
summary(logit.reg)
```
## For variables with NA (self-employed and unknown), they are the defaults dummy values; just like genderFemale is the default dummy of the regression and ever_married_No is the default dummy for ever_married:
- The effect of being male is positive => male have higher propensity of stroke
- The coefficients effect of the work_type dummies are in comparison to the default value unknown (which is why it has NA).
- The coefficients effect of the smoking_status dummies are in comparison to the default value self-employed (with NA).

The significant predictors are in order (from most influencer to least) age, hypertension and avg_glucose_level.

```{r}
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = Train, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(Train, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)

ggplot(Valid) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = Train, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(Train, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)

ggplot(Valid) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For average glucose level:
mod3 <- glm(stroke ~ avg_glucose_level, data = Train, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(Train, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)

ggplot(Valid) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
```

- Age: The older you get, the more probable it is you get a stroke. The first instances of stroke arises around 30 year of age according to the graph; and it start rising the older you get.
- Average glucose: the progression of the slope is slower and way less impactful than age; this is to be expected as the variable avg_glucose_level has lower significance in the model.

```{r}
# Confusion matrix
pred.lr = predict(logit.reg, Valid[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid$stroke, positive = "1")$table)
```

This model is not able to predict any stroke; it is not efficient for the goal of our analysis. It is most probable that we have too many unnecessary variables in our model:

##########################################
```{r}
logit.reg <- glm(stroke ~ age + hypertension + avg_glucose_level, data = Train_norm, family = "binomial")
options(scipen = 999)
summary(logit.reg)

pred.lr = predict(logit.reg, Valid_norm[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid_norm$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid_norm$stroke, positive = "1")$table)
```
