---
title: "Project"
subtitle: "Creating Value Through Data Mining"
author: "Malou-Tinette Kouango & Veronica Rowe"
date: "`r Sys.Date()`"
output: html_document
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Stroke Prediction Dataset 
# Clearing the console
```{r, warning = F,message = F, echo = TRUE}
#clear all the console
list=ls()
rm(list=ls()) 
cat("\014")
ls()
```

#Reading the Stroke dataset
```{r}
Stroke <- read.csv(file.choose())
```

# Loading the relevant packages
```{r, message =FALSE, warning=FALSE}
library(ggplot2) 
library(FNN)
library(dummies)
library(e1071)
library (caret)
library(reshape2)
library(dplyr)
library(class)
library(naniar)
require(ellipse)
```

 
### Exposing the data structure
```{r}
# Types of variables:
str(Stroke)
```

### We have 12 variables in the Stroke dataset, each having different types. Our outcome variable "Stroke" is a binary outcome, taking value 1 if the person had a stroke and 0 if they haven't had a stroke. => change it as factor
### Bmi is seen as a character because of the N/A => we must change N/A strings to real NA in R, and set the variable as numeric to be able to visualize it in our exploratory graphs.
### The variables gender, ever_married, work_type, Residence_type and smoking status are seen as character, when they are categorical => we must change them to factor for better data visualization.
### The variable id (= id of the patient in the hospital) is irrelevant to our analysis => we remove it

```{r}
# Changing N/A characters in bmi to real NA:
Stroke = replace_with_na(Stroke, replace = list(bmi = "N/A")) # bmi has NA now
# Change bmi from character to numerical variable:
Stroke$bmi <- as.numeric(Stroke$bmi)
Stroke$stroke <- as.factor(Stroke$stroke)

# Change character variables as categorical (factor)
Stroke[,c(2,6:8,11)] <- lapply(Stroke[,c(2,6:8,11)], as.factor)

# Remove id row
Stroke = Stroke[,-1]
str(Stroke)
```


```{r}
# Show how many different values there is for each variable
sapply(Stroke, function(x) length(unique(x)))
summary(Stroke)
```
#### Using sapply function from TP1 to find out more about the variables. We realise that in gender, there is only one observation as categorised as "Other". 
### To simplify our analysis, we will later disregard the row with that observation, as it not significant enough to be taken into consideration (1/5110 = 0.01956947%) (row/observation number id = 56'156/ row = 3117). We will also disregard the ID column as it only represent the patient id in the hospital and thus has no impact on our outcome variable of interest Stroke.

## Finding out information on our outcome variable
```{r}
table(Stroke$stroke)
```
#### We see that 249 people had a stroke and 4861 did not. We have an unbalanced binary outcome. => we must put results in context


## Visualising numerical variables. 
```{r}
# Visualize the data:
# gender:
ggplot(data = Stroke, aes(x = gender, fill = stroke)) + geom_bar() + ggtitle("Gender vs Stroke")
# age:
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_boxplot() + ggtitle("Age vs Stroke")
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_histogram() + ggtitle("Age vs Stroke")
# hypertension:
ggplot(data = Stroke, aes(x = hypertension, fill = stroke)) + geom_bar() + ggtitle("Hypertension vs Stroke")
# heart disease:
ggplot(data = Stroke, aes(x = heart_disease, fill = stroke)) + geom_bar() + ggtitle("Heart disease vs Stroke")
# ever married:
ggplot(data = Stroke, aes(x = ever_married, fill = stroke)) + geom_bar() + ggtitle("Ever married vs Stroke")
# work type:
ggplot(data = Stroke, aes(x = work_type, fill = stroke)) + geom_bar() + ggtitle("Work type vs Stroke")
# Residence type:
ggplot(data = Stroke, aes(x = Residence_type, fill = stroke)) + geom_bar() + ggtitle("Residence type vs Stroke")
# Average glucose level:
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_boxplot() + ggtitle("Average glucose level vs Stroke")
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_histogram() + ggtitle("Average glucose level vs Stroke")
# BMI:
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_boxplot() + ggtitle("BMI vs Stroke")
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_histogram() + ggtitle("BMI vs Stroke")
# Smoking status:
ggplot(data = Stroke, aes(x = smoking_status, fill = stroke)) + geom_bar() + ggtitle("Smoking status vs Stroke")
```

#### In the histograms, we see that the average age of the patient is in his 40ties and the average glucose and bmi are right-skewed.
First of all, our data visualisation are our assumptions without having explored or analysed the significance of the explanatory variables. Moreover, the outcome variable stoke is unbalanced so we should be wary of these assumptions. We are visualising them & identifying potential explanations in order to have an idea before  going further. 

We see that the levels of males/females having strokes are about the same. However, this needs to be scaled as we also see that there are more women in the sample than men. It is safe to say that men have a higher chance of having a stroke (yet to be determined the effect/in which significance level). 

From the boxplot of Age variable. It is undeniable that the older the person, the higher are their chances of having a stroke (blue boxplot more to the right) and an important difference between both means of both groups. This can be proved also on our histogram. 

Hypertension seems to have an effect, again the scaling is problematic but in relative terms, a lot of patients who suffered a stroke had hypertension. The same for the heart disease but in a way lower importance (blue line in 1 column is less prevalent than in hypertension).

Marriage status, strokes seem to be more common in the people that were/are married. This is due to underlying assumptions, usually married people are older (so maybe it is correlated with age, hypertension, heart disease, that have a higher probability to be the case, denoted by a result of 1, for older people). 

As for the working status, we see that private work type seems to accentuate the likelihood of a strike, however, taking into consideration, the number of observations of that category is way higher so it is logical that stroke probability is also higher. Drop o pas drop ? 

Residence type is pretty evenly distributed and there seems to be a little difference between both categories, therefore, we have decided to drop this variable. 

For the average glucose, We see that a higher level indicates a higher chance of getting a stroke. It is fair to assume that excessive sugar levels are not healthy. This comment is also valid for our dataset.  This same idea is also in the BMI variable, where the difference is not so flagrant but again, medically BMI (even if it is contested) does indicate something about the patient's health status. 

As for the smoking variable, the scaling is again problematic as our outcome variable is unbalanced, we will have to further investigate in order to see the actual impact it has on the likelihood of having a stroke. 

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Summary 
We see that all "medical terms" (hypertension, heart disease, glucose level, BMI, smoking, age)  most of the time have an impact. It is commonly known that the general health status of a person will have an effect on the likelihood of having a stroke/heart attack and other illnesses. Usually, the weaker a person is the higher the chance of them getting sick. This assumption is pretty much satisfied in our data. 


We see in the more "social terms" (gender, age, marriage status, work type, residence type). These variables have an impact on the age of the person (that is how they are directly linked to the medical terms & why they can be significant and should be taken into consideration before being potentially taken out by the model). Marriage status is the most clear example as older people usually have a higher chance of being married, the same goes for work type with the "children/never_worked categories connected to a young age" . It must be estimated and analysed in order to not assume spurious correlations. We need the data to back up any assumptions.


#### Boxplot visualisation
```{r}
par(mfrow = c(2, 3))
boxplot(id~hypertension,data = Stroke, main = "Hypertension distribution")
boxplot(id~heart_disease,data = Stroke,  main = "Heart disease distribution")
boxplot(id~ever_married,data = Stroke,  main = "Marriage status distribution")
boxplot(id~work_type,data = Stroke,  main = "Work Type distribution")
boxplot(id~Residence_type,data = Stroke,  main = "Residence type distribution")
boxplot(id~smoking_status,data = Stroke,  main = "Smoking status distribution")
boxplot(id~stroke,data = Stroke,  main = "Stroke distribution")
```
#### We see that most variables are evenly distributed, roughly same weights ("50% of the sample have the account and the other 50% have not, for the variables separated into 2 categories"). We see that there is a slight majority of people not accepting the Personal Loan (mean of 0 is higher than the one of 1). I am skeptical of these results as above we saw that less than 10% took the Loan(PL is 1), however in the boxplot it seems like thez are nearlz on the same level.

#### Preparing for analysis : 

````{r}
# Remove only row with gender = "Other"
Stroke = Stroke[-3117,]
```


#### Transforming the categorical variables with more than 2 categories into dummy variables (Here it concerns variables work_type (6) & smoking_status (10))

```{r}
set.seed(1)
Stroke2 = Stroke
# Put work_type as dummy:
work_type_Dummies <- cbind( Stroke[1:5], dummy(Stroke$work_type, sep = "_"), Stroke[7:11])
names(work_type_Dummies)[6:10] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
work_type_Dummies
Stroke2 <- work_type_Dummies
```

```{r}
# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke2[1:13], dummy(Stroke2$smoking_status, sep = "_"), Stroke2[15])
names(smoking_status_Dummies)[14:17] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
smoking_status_Dummies
Stroke2 <- smoking_status_Dummies
```


#### Partitioning the 2 groups (60% in Training, 40% in Valid) & normalizing the data 
```{r}
set.seed(1)
Train <- sample(row.names(Stroke), 0.6*dim(Stroke)[1])
Valid <- setdiff(row.names(Stroke), Train)
Train <- Stroke[Train, ]
Valid <- Stroke[Valid, ]
dim(Train)
dim(Valid)
# The data has been correctly partitioned into 60% (3'065 obs) in T and 40% in V (2'044 obs)
```


#### Initializing training, validation data, using preProcess()
```{r}
# initialize normalized training and validation sets:
Train_norm <- Train
Valid_norm <- Valid
Stroke_normalized <- Stroke

# use preProcess() from the caret package to normalize the variables age, average_glucose and bmi:
Value.norm <- preProcess(Train[, c(2,8,9)], method= "range")

Train_norm[, c(2,8,9)] <- predict(Value.norm, Train[, c(2,8,9)])
Valid_norm[, c(2,8,9)] <- predict(Value.norm, Valid[, c(2,8,9)])
Stroke_normalized[, c(2,8,9)] <- predict(Value.norm, Stroke[, c(2,8,9)])
```

```{r}
# 1) Logistic Regression
logit.reg <- glm(stroke ~., data = Train, family = "binomial")
options(scipen = 999)
summary(logit.reg)
```
## For variables with NA (self-employed and unknown), they are the defaults dummy values; just like genderFemale is the default dummy of the regression and ever_married_No is the default dummy for ever_married:
- The effect of being male is positive => male have higher propensity of stroke
- The coefficients effect of the work_type dummies are in comparison to the default value unknown (which is why it has NA).
- The coefficients effect of the smoking_status dummies are in comparison to the default value self-employed (with NA).

The significant predictors are in order (from most influencer to least) age, hypertension and avg_glucose_level.

```{r}
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = Train, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(Train, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)

ggplot(Valid) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = Train, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(Train, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)

ggplot(Valid) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For average glucose level:
mod3 <- glm(stroke ~ avg_glucose_level, data = Train, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(Train, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)

ggplot(Valid) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
```

- Age: The older you get, the more probable it is you get a stroke. The first instances of stroke arises around 30 year of age according to the graph; and it start rising the older you get.
- Average glucose: the progression of the slope is slower and way less impactful than age; this is to be expected as the variable avg_glucose_level has lower significance in the model.

```{r}
# Confusion matrix
pred.lr = predict(logit.reg, Valid[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.04, 1, 0)), Valid$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.04, 1, 0)), Valid$stroke, positive = "1")$table)
```

This model is not able to predict any stroke; it is not efficient for the goal of our analysis. It is most probable that we have too many unnecessary variables in our model.

The model's goal is to predict stroke. As such, a false positive (saying a patient is likely to get a stroke when they are not) is better than a false negative (saying a patient will not get a stroke when they are actually likely). Thus, sensitivity (rate of true positive) is more important than both specificity (rate of true negative); because detecting a false stroke have less negative consequences than not detecting it (preventive medical monitoring cannot hurt, but not providing them when it could have been useful does).


FIND THE CUTOFF THAT MAXIMIZES SENSITIVITY and the overall ACCURACY !!!!!!!!!!
```{r}
cut_off = seq(0.01,0.99,0.01)
metrics.df <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))

for (i in 1:length(cut_off)){
  x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
  metrics.df[i, 2] <- confusionMatrix(x, Valid$stroke, positive = "1")$byClass[1]
  metrics.df[i, 3] <- confusionMatrix(x, Valid$stroke, positive = "1")$overall[1]
}

cutoff.plot = ggplot(data= metrics.df) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.044 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")

cutoff.plot
metrics.df$difference = sqrt((metrics.df$accuracy - metrics.df$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.df$difference)]
cut_off.best
```


##########################################
```{r}
logit.reg <- glm(stroke ~ age + hypertension + avg_glucose_level, data = Train_norm, family = "binomial")
options(scipen = 999)
summary(logit.reg)

pred.lr = predict(logit.reg, Valid_norm[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid_norm$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), Valid_norm$stroke, positive = "1")$table)
```
