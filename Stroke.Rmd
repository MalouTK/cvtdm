---
title: "Project"
subtitle: "Creating Value Through Data Mining"
author: "Malou-Tinette Kouango & Veronica Rowe"
date: "`r Sys.Date()`"
output: html_document
--- 

NOTE:


---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Project- Stroke Prediction Dataset 
# Clearing the console
```{r, warning = F,message = F, echo = TRUE}
#clear all the console
list=ls()
rm(list=ls()) 
cat("\014")
ls()
```

#Reading the Stroke dataset
```{r}
Stroke <- read.csv(file.choose())
```

# Loading the relevant packages
```{r, message =FALSE, warning=FALSE}
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(imputeTS)
library(FNN)
library(dummies)
library(e1071)
library (caret)
library(reshape2)
library(dplyr)
library(class)
library(naniar)
require(ellipse)
library(cowplot)
library(rpart)
library(rpart.plot)
library(MASS)
library(DiscriMiner)
```

 
#### EDA- Explortory Data Analysis
```{r}
# Types of variables:
str(Stroke)
```

We have 12 variables in the Stroke dataset, each having different types. Our outcome variable "Stroke" is a binary outcome, taking value 1 if the person had a stroke and 0 if they haven't had a stroke. => change it as factor
- Bmi is seen as a character because of the N/A => we must change N/A strings to real NA in R, and set the variable as numeric to be able to visualize it in our exploratory graphs.
- The variables gender, ever_married, work_type, Residence_type and smoking status are seen as character, when they are categorical => we must change them to factor for better data visualization.
- The variable id (= id of the patient in the hospital) is irrelevant to our analysis => we remove it

```{r}
# Changing N/A characters in bmi to real NA:
Stroke = replace_with_na(Stroke, replace = list(bmi = "N/A")) # bmi has NA now
# Change bmi from character to numerical variable:
Stroke$bmi <- as.numeric(Stroke$bmi)
Stroke$stroke <- as.factor(Stroke$stroke)

# Change character variables as categorical (factor)
Stroke[,c(2,6:8,11)] <- lapply(Stroke[,c(2,6:8,11)], as.factor)

# Remove id row
Stroke = Stroke[,-1]
str(Stroke)
```


```{r}
# Show how many different values there is for each variable
sapply(Stroke, function(x) length(unique(x)))
summary(Stroke)
```
Using sapply function from TP1 to find out more about the variables. We realise that in gender, there is only one observation as categorised as "Other". 

To simplify our analysis, we will later disregard the row with that observation, as it not significant enough to be taken into consideration (1/5110 = 0.01956947%) (row/observation number id = 56'156/ row = 3117). We will also disregard the ID column as it only represent the patient id in the hospital and thus has no impact on our outcome variable of interest Stroke.

Finding out information on our outcome variable
```{r}
table(Stroke$stroke)
ratio_0 = 4861/(4861+249)
ratio_1 = 1 - ratio_0
ratio_0
ratio_1
```
We see that 249 people had a stroke and 4861 did not. We have an unbalanced binary outcome, with 4861/(4861+249) = 95.1272% non-stroke and only 4.8728% of stroke occurrences.


# Data Visualization
```{r message=FALSE, warning=FALSE}
# Visualize the data:
# gender:
ggplot(data = Stroke, aes(x = gender, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Gender vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# age:
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_boxplot() + ggtitle("Age vs Stroke")
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_histogram() + ggtitle("Age vs Stroke")
# hypertension:
ggplot(data = Stroke, aes(x = hypertension, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Hypertension vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# heart disease:
ggplot(data = Stroke, aes(x = heart_disease, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Heart disease vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# ever married:
ggplot(data = Stroke, aes(x = ever_married, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Ever married vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# work type:
ggplot(data = Stroke, aes(x = work_type, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Work type vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# Residence type:
ggplot(data = Stroke, aes(x = Residence_type, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Residence type vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# Average glucose level:
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_boxplot() + ggtitle("Average glucose level vs Stroke")
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_histogram() + ggtitle("Average glucose level vs Stroke")
# BMI:
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_boxplot() + ggtitle("BMI vs Stroke")
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_histogram() + ggtitle("BMI vs Stroke")
# Smoking status:
ggplot(data = Stroke, aes(x = smoking_status, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Smoking status vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
```
In the histograms, we see that the average age of the patient is in his 40ties and the average glucose and bmi are right-skewed.

First of all, our data visualisation are our assumptions without having explored or analysed the significance of the explanatory variables. Moreover, the outcome variable stoke is unbalanced so we should be wary of these assumptions. We are visualising them & identifying potential explanations in order to have an idea before  going further. 

We see that the levels of males/females having strokes are about the same. However, this needs to be scaled as we also see that there are more women in the sample than men. It is safe to say that men have a higher chance of having a stroke (yet to be determined the effect/in which significance level). 

From the boxplot of Age variable. It is undeniable that the older the person, the higher are their chances of having a stroke (blue boxplot more to the right) and an important difference between both means of both groups. This can be proved also on our histogram. 

Hypertension seems to have an effect, again the scaling is problematic but in relative terms, a lot of patients who suffered a stroke had hypertension. The same for the heart disease but in a way lower importance (blue line in 1 column is less prevalent than in hypertension).

Marriage status, strokes seem to be more common in the people that were/are married. This is due to underlying assumptions, usually married people are older (so maybe it is correlated with age, hypertension, heart disease, that have a higher probability to be the case, denoted by a result of 1, for older people). 

As for the working status, we see that private work type seems to accentuate the likelihood of a strike, however, taking into consideration, the number of observations of that category is way higher so it is logical that stroke probability is also higher. Drop o pas drop ? 

Residence type is pretty evenly distributed and there seems to be a little difference between both categories, therefore, we have decided to drop this variable. 

For the average glucose, We see that a higher level indicates a higher chance of getting a stroke. It is fair to assume that excessive sugar levels are not healthy. This comment is also valid for our dataset.  This same idea is also in the BMI variable, where the difference is not so flagrant but again, medically BMI (even if it is contested) does indicate something about the patient's health status. 

As for the smoking variable, the scaling is again problematic as our outcome variable is unbalanced, we will have to further investigate in order to see the actual impact it has on the likelihood of having a stroke. 

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Summary 
We see that all "medical terms" (hypertension, heart disease, glucose level, BMI, smoking, age)  most of the time have an impact. It is commonly known that the general health status of a person will have an effect on the likelihood of having a stroke/heart attack and other illnesses. Usually, the weaker a person is the higher the chance of them getting sick. This assumption is pretty much satisfied in our data.

We see in the more "social terms" (gender, age, marriage status, work type, residence type). These variables have an impact on the age of the person (that is how they are directly linked to the medical terms & why they can be significant and should be taken into consideration before being potentially taken out by the model). Marriage status is the most clear example as older people usually have a higher chance of being married, the same goes for work type with the "children/never_worked categories connected to a young age" . It must be estimated and analysed in order to not assume spurious correlations. We need the data to back up any assumptions.

# Correlation plots
```{r}
Stroke.corr <- Stroke
Stroke.corr[,1] <- as.numeric(factor(Stroke.corr[,1]))
Stroke.corr[,2] <- as.numeric(factor(Stroke.corr[,2]))
Stroke.corr[,3] <- as.numeric(factor(Stroke.corr[,3]))
Stroke.corr[,4] <- as.numeric(factor(Stroke.corr[,4]))
Stroke.corr[,5] <- as.numeric(factor(Stroke.corr[,5]))
Stroke.corr[,6] <- as.numeric(factor(Stroke.corr[,6]))
Stroke.corr[,7] <- as.numeric(factor(Stroke.corr[,7]))
Stroke.corr[,8] <- as.numeric(factor(Stroke.corr[,8]))
Stroke.corr[,9] <- as.numeric(factor(Stroke.corr[,9]))
Stroke.corr[,10] <- as.numeric(factor(Stroke.corr[,10]))
Stroke.corr[,11] <- as.numeric(factor(Stroke.corr[,11]))

corrplot(cor(na.omit(Stroke.corr)), method = "number")
corr <- round(cor(na.omit(Stroke.corr)), 1)  
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of Stroke", 
           ggtheme=theme_bw)
heatmap(x = corr, col = c("green", "white", "red"), symm = TRUE)
```


```{r, warning=FALSE, message=FALSE}
#Stroke.corr <- Stroke.fltr
#Stroke.corr$stroke <- as.numeric(Stroke.corr$stroke)
#Stroke.corr <- na.omit(Stroke.corr)
#corr <- round(cor(Stroke.corr), 1)  
#ggcorrplot(corr, hc.order = TRUE,type = "lower", lab = TRUE, lab_size = 2, method="circle", colors = c("tomato2", "white", "springgreen3"), title="Correlogram of Stroke", ggtheme=theme_bw)
#heatmap(x = corr, col = c("green", "white", "red"), symm = TRUE)
```

# Preparing the data for analysis : 

````{r}
# Common data standardization
# Remove only row with gender = "Other"
Stroke = Stroke[-3117,]
# Set 2 levels for gender (and not 3):
Stroke$gender <- factor(Stroke$gender, levels = c("Female", "Male"))

# Get rid of correlated variables: ever_married and work_type_children:
#Stroke[,-5]

# Replace NAs with mean value (column bmi):
Stroke <- na_mean(Stroke)

# Omit rows with NAs in bmi (since LR and KNN does not consider NAs):
#Stroke.withNA = Stroke  # keep a copy with NAs
#Stroke <- na.exclude(Stroke)
```

#### 1) Logistic Regression

# Partition for LR: 

We split 60%-Training, 40%-Validation and normalize the data 
```{r}
set.seed(1)
train.lr <- sample(row.names(Stroke), 0.6*dim(Stroke)[1])
valid.lr <- setdiff(row.names(Stroke), train.lr)
train.lr <- Stroke[train.lr, ]
valid.lr <- Stroke[valid.lr, ]
dim(train.lr)
dim(valid.lr)
# The data has been correctly partitioned into 60% (3'065 obs) in T and 40% in V (2'044 obs)
```
# Doing LR

```{r}
set.seed(1)
# 1) Logistic Regression
logit.reg <- glm(stroke ~., data = train.lr, family = "binomial")
options(scipen = 999)
summary(logit.reg)
```
For variables with NA (self-employed and unknown), they are the defaults dummy values; just like genderFemale is the default dummy of the regression and ever_married_No is the default dummy for ever_married:

- The effect of being male is positive => male have higher propensity of stroke
- The coefficients effect of the work_type dummies are in comparison to the default value unknown (which is why it has NA).
- The coefficients effect of the smoking_status dummies are in comparison to the default value self-employed (with NA).

The significant predictors are in order (from most influencer to least) age, hypertension and avg_glucose_level.

```{r}
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = train.lr, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(train.lr, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)

ggplot(valid.lr) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = train.lr, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)

ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()

# For average glucose level:
mod3 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)

ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
```

- Age: The older you get, the more probable it is you get a stroke. The first instances of stroke arises around 30 year of age according to the graph; and it start rising the older you get.
- Average glucose: the progression of the slope is slower and way less impactful than age; this is to be expected as the variable avg_glucose_level has lower significance in the model.

```{r}
# Confusion matrix with cut-off 0.5:
set.seed(1)
pred.lr = predict(logit.reg, valid.lr[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
```

This model is not able to predict any stroke; it is not efficient for the goal of our analysis. It is most probable that we have too many unnecessary variables in our model.

The model's goal is to predict stroke. As such, a false positive (saying a patient is likely to get a stroke when they are not) is better than a false negative (saying a patient will not get a stroke when they are actually likely). Thus, sensitivity (rate of true positive) is more important than both specificity (rate of true negative); because detecting a false stroke have less negative consequences than not detecting it (preventive medical monitoring cannot hurt, but not providing them when it could have been useful does).


Find the cutoff that maximizes both sensitivity and overall accuracy:
```{r, warning=FALSE, message=FALSE}
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))

for (i in 1:length(cut_off)){
  x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
  metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
  metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}

cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.0484 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")

cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
```

The best cut-off is 0.05; which is close to the ratio of stroke/non-stroke in presence in this hospital's dataset: 4.8728%.

```{r}
# Confusion matrix with best cut-off (0.4):
conf.lr = confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
```
#### 2) Classification Tree

# Partition for Classification Tree 

We partition into 60/40 and put categorical variables with more than 2 levels as dummies:
```{r, warning=FALSE, message=FALSE}
set.seed(1)
Stroke.dummy = Stroke   # Stroke.withNA

# Put work_type as dummy:
work_type_Dummies <- cbind(Stroke.dummy[1:5], dummy(Stroke.dummy$work_type, sep = "_"), Stroke.dummy[7:11])
names(work_type_Dummies)[6:10] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
#work_type_Dummies
Stroke.dummy <- work_type_Dummies

# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke.dummy[1:13], dummy(Stroke.dummy$smoking_status, sep = "_"), Stroke.dummy[15])
names(smoking_status_Dummies)[14:17] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
#smoking_status_Dummies
Stroke.dummy <- smoking_status_Dummies

train.index.ct = sample(NROW(Stroke.dummy), round(NROW(Stroke.dummy)*0.6))
train.ct = Stroke.dummy[train.index.ct,]
valid.ct = Stroke.dummy[- train.index.ct,]
```

# Doing CT

```{r}
# Full-grown tree:
set.seed(1)
c.tree = rpart(stroke ~., data = train.ct, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5, parms = list(prior= c(ratio_0, ratio_1)))
length(c.tree$frame$var[c.tree$frame$var == "<leaf>"])
c.tree$variable.importance
prp(c.tree)
```

```{r}
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)

prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp

p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
```

```{r}
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
```

Again, the model is not able to accurately predict strokes due to a highly unbalanced dataset. We must find the best cp, such that it maximizes both sensitivity and accuracy for the model to be able to predict stroke (according to our goal).

```{r}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")

row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))

for (i in 1:length(row.cp)){
  set.seed(1)
  pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
  x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
  metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
  metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# error: predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))


cp.plot = ggplot(data= metrics.ct) + geom_line(aes(x=row.cp,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=row.cp,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.001 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cp row")

cp.plot
metrics.ct$difference = sqrt((metrics.ct$accuracy - metrics.ct$sensitivity)^2)
row.cp.best = row.cp[which.min(metrics.ct$difference)]
cp.best = c.tree$cptable[,"CP"][row.cp][row.cp.best]
cp.best

set.seed(1)
best.pruned <- prune(c.tree, cp = cp.best)
pred.best.pruned <- predict(best.pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
prp(best.pruned)
```
The best pruned tree for predictive stroke is the full classification tree.

```{r}
# Confusion matrix for CT:
conf.ct = confusionMatrix(pred.best.pruned, valid.ct$stroke, positive = "1")
conf.ct
fourfoldplot(confusionMatrix(pred.best.pruned, valid.ct$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))

# Prediction on the training set itself:
#confusionMatrix(predict(best.pruned, train.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1))), train.ct$stroke, positive = "1")
```

#### 3) KNN

# Partition for KNN

We partition into 60/40, create dummy variables, change all predictors to numerical and normalize them:
````{r}
# Dummify:
Stroke$stroke <- as.integer(ifelse(Stroke$stroke == "0", 0,1))  # change stroke to int during dummification
dummies = dummyVars(~ ., data=Stroke)
Stroke.fltr = as.data.frame(predict(dummies, newdata = Stroke)[,-11])
Stroke.fltr$stroke = as.factor(Stroke.fltr$stroke)  # change stroke back to factor
str(Stroke.fltr)
```

```{r}
prop.table(table(Stroke.fltr$stroke))

set.seed(1)
train.index.knn = sample(NROW(Stroke.fltr), round(NROW(Stroke.fltr)*0.6))
train.knn = Stroke.fltr[ train.index.knn,]
valid.knn  = Stroke.fltr[-train.index.knn,]
print(table(train.knn$stroke) / table(Stroke.fltr$stroke)) #Training set partition
print(table(valid.knn$stroke) / table(Stroke.fltr$stroke)) #Validation set partition
```

```{r, warning=FALSE, message=FALSE}
# Normalize the dummified sets:
train.norm.knn = train.knn
valid.norm.knn = valid.knn
# Normalization of numerical variables:
norm.values = preProcess(train.knn[, c(3,14,15)], method=c("range"))
train.norm.knn[, c(3,14,15)] = predict(norm.values, train.knn[, c(3,14,15)])
valid.norm.knn[, c(3,14,15)] = predict(norm.values, valid.knn[, c(3,14,15)])
```

# Doing KNN

```{r}
# KNN does not tolerate NAs: omit them
#train.norm.knn <- na.omit(train.norm.knn)
#valid.norm.knn <- na.omit(valid.norm.knn)

### Creating the graph for best K. (column 21/22 = stroke0/stroke1)

set.seed(1)
nk = 10
metrics.knn <- data.frame(k = seq(1, nk, 1), accuracy = rep(0, nk), sensitivity = rep(0, nk), F1_score = rep(0, nk))

for(i in 1:nk) {
  knn.pred = knn(train.norm.knn[, -20], valid.norm.knn[, -20], cl = train.norm.knn[, 20], k = i)
  metrics.knn[i, 2] = confusionMatrix(knn.pred, valid.norm.knn[, 20], positive = "1")$overall[1]
  metrics.knn[i, 3] = confusionMatrix(knn.pred, valid.norm.knn[, 20], positive = "1")$byClass[1]
}

accuracy.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=accuracy), color="darkred") + theme_classic()
sensitivity.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=sensitivity), color="blue") + theme_classic()
both.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=accuracy), color="darkred") + geom_line(aes(x=k,y=sensitivity))+ ylab("Both") + theme_classic()

plot_grid(accuracy.plot, sensitivity.plot, both.plot, ncol = 1, align = "v")

# Compute best trade-off sensitivity-accuracy
metrics.knn$difference = sqrt((metrics.knn$accuracy - metrics.knn$sensitivity)^2)
k.best = metrics.knn$k[which.min(metrics.knn$difference)]
k.best
```
The best k is 1, which is logical for such an unbalanced dataset. Indeed, the higher the value of k (= number of reference neighbors), the higher the chance of belonging to the majority group, especially in our case where non-stroke represents 95% of observations. So in case of unbalanced dataset, instead of faulting the results (with a bias due to a lack of diversity), a lower k helps being more precise.

```{r}
### Creating the confusion Matrix --> need just to change the K = 1 & the 21 corresponds to the stroke variable. 

pred.knn = knn(train.norm.knn[, -20], valid.norm.knn[, -20], cl = train.norm.knn[, 20], k = k.best, prob = T) 
## k.best = 1:
conf.knn = confusionMatrix(pred.knn, valid.norm.knn[, 20], positive = "1")
conf.knn
fourfoldplot(confusionMatrix(pred.knn, valid.norm.knn[, 20], positive = "1")$table, color = c("#CC6666", "#99CC99"))
```

## 4) Ensemble methods

```{r}
pred.ct.prob = predict(best.pruned, valid.ct, type = "prob", parms = list(prior= c(ratio_0, ratio_1)))[,2]

pred.knn.prob = attr(pred.knn, "prob")
  
# LR:
results.df = data.frame(actual = valid.lr$stroke, lr.binary = as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), lr.prob = pred.lr, ct.binary = pred.ct, ct.prob = pred.ct.prob, knn.binary = pred.knn, knn.prob = pred.knn.prob)

for(i in 1:NROW(results.df)){
  fix_prob = if(results.df[i,"knn.binary"]==1) results.df[i,"knn.prob"] else 1 - results.df[i,"knn.prob"]
  results.df[i,"knn.prob"] = fix_prob
}

accuracy.df = data.frame("LR.Accuracy" = conf.lr$overall[1], "CT.Accuracy" = conf.ct$overall[1], "KNN.Accuracy" = conf.knn$overall[1])

sensitivity.df = data.frame("LR.Sensitivity" = conf.lr$byClass[1], "CT.Sensitivity" = conf.ct$byClass[1], "KNN.Sensitivity" = conf.knn$byClass[1])
```

```{r}
# Ensembles:
results.df$average.prob = apply(results.df[,c(3,5,7)],FUN = mean, MARGIN = 1)
results.df[,c(2,4,6)] = sapply(results.df[,c(2,4,6)], as.integer) - 1
results.df$major.voting = as.factor(apply(results.df[,c(2,4,6)],FUN = function(x) 
names(table(melt(x)$value))[which.max(table(melt(x)$value))], MARGIN = 1))

# Majority vote (ensemble):
accuracy.df = cbind(accuracy.df, ensemble = confusionMatrix(results.df$major.voting, results.df$actual, positive = "1")$overall[1])
sensitivity.df = cbind(sensitivity.df, ensemble = confusionMatrix(results.df$major.voting, results.df$actual, positive = "1")$byClass[1])
```

```{r, warning=FALSE, message=FALSE}
# Average (ensemble):
# Find the cutoff for average (ensemble): that maximizes both sensitivity and overall accuracy

# Find the best ensemble cut-off:
cut_off.ens = seq(0.01,0.99,0.01)
metrics.ens <- data.frame(cut_off.ens, sensitivity = rep(0, length(cut_off.ens)), accuracy = rep(0, length(cut_off.ens)))

for (i in 1:length(cut_off.ens)){
  x <- as.factor(ifelse(results.df$average.prob >= cut_off.ens[i],1,0))
  metrics.ens[i, 2] <- confusionMatrix(x, results.df$actual, positive = "1")$byClass[1]
  metrics.ens[i, 3] <- confusionMatrix(x, results.df$actual, positive = "1")$overall[1]
}

cutoff.plot.ens = ggplot(data= metrics.ens) + geom_line(aes(x=cut_off.ens,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off.ens,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.019 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")

cutoff.plot.ens
metrics.ens$difference = sqrt((metrics.ens$accuracy - metrics.ens$sensitivity)^2)
cut_off.best.ens = cut_off.ens[which.min(metrics.ens$difference)]
cut_off.best.ens


accuracy.df = cbind(accuracy.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$overall[1])
sensitivity.df = cbind(sensitivity.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$byClass[1])
```


```{r, warning=FALSE, message=FALSE}
accuracy.df.melt = melt(accuracy.df)
accuracy.plot = ggplot(aes(x=reorder(variable, value), y=value), data = accuracy.df.melt) + geom_col(fill="darkblue") + ylab("accuracy") + xlab("model") +  
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic() 
accuracy.plot

sensitivity.df.melt = melt(sensitivity.df)
sensitivity.plot = ggplot(aes(x=reorder(variable, value), y=value), data = sensitivity.df.melt) + geom_col(fill="darkblue") + ylab("sensitivity") + xlab("model") +  
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic() 
sensitivity.plot
```

```{r}
acc = c(accuracy.df[1,1], accuracy.df[1,2], accuracy.df[1,3], accuracy.df[1,4], accuracy.df[1,5])
sens = c(sensitivity.df[1,1], sensitivity.df[1,2], sensitivity.df[1,3], sensitivity.df[1,4], sensitivity.df[1,5])
method = c("LR", "CT", "KNN", "Ensemble", "Ensemble.prob")
both.df = data.frame(method, "accuracy"= acc, "sensitivity"=sens)

both.df$sum = both.df$accuracy + both.df$sensitivity

#barplot(sum ~ method, both.df, main = "Methods overall comparison", ylab = "accuracy + sensitivity", xlab = "", las = 2)

both.plot = ggplot(aes(x=method, y=sum), data = both.df) + geom_col(fill = "darkgreen") + ylab("sum") + xlab("model") + ggtitle("Methods overall comparison") + geom_text(aes(label = round(sum,4)), colour = "grey38", size = 4, vjust = -0.3) + theme_classic() 
both.plot
```

COMPARISON OF ALL METHODS:
As the results of the full models of the CT and KNN are so low in sensitivity, there are inadequate for this highly unbalanced dataset. Therefore, our assumption was that doing a ensemble method by majority & average would be counterproductive, as the results yield would be lower than the logistic regression.
The ensemble prob method, that allows for a cutoff, is actually much better than we thought. Its balanced predictive power is good, though still lower than LR.
=> Blindly, ensemble methods allow for a good result.


## 5) Best model: Prediction

LR is the best fitted model => use this method as the basis for prediction.

```{r}
set.seed(1)
# Reduced GLM
logit.reg2 <- glm(stroke ~ age + hypertension + avg_glucose_level, data = train.lr, family = "binomial")
options(scipen = 999)
summary(logit.reg2)

pred.lr2 = predict(logit.reg2, valid.lr[,-18], type = "response")
# Confusion matrix with best cut-off (0.4):
conf.lr2 = confusionMatrix(as.factor(ifelse(pred.lr2 > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr2
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr2 > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))

lr.plots = both.df[1,c(1,4)]
lr.plots[2,1] = c("LR.reduced")
lr.plots[2,2] = conf.lr2$overall[1]+conf.lr2$byClass[1]

lr.plots.plot = ggplot(aes(x=method, y=sum), data = lr.plots) + geom_col(fill = "darkviolet") + ylab("sum") + xlab("model") + ggtitle("LR VS reduced LR") + geom_text(aes(label = round(sum,4)), colour = "grey38", size = 4, vjust = -0.3) + theme_classic() 
lr.plots.plot
```
The reduced model has 1 more stroke prediction, for ~30 less accurate non-stroke predictions



################################################################################



4) LDA - Linear Discriminant Analysis

The significant predictors are in order (from most influencer to least) age, hypertension and avg_glucose_level.

```{r}
# Create a test set - 10% of dataset:
set.seed(1)
Test <- sample(row.names(Stroke), 0.1*dim(Stroke)[1])
Test <- Stroke[Test, ]
Test <- Test[,c(2,3,11)]
print(table(Test$stroke) / table(Stroke$stroke))

# Draw a colored scatterplot of Experience vs Training, with ggplot2-
# - with Training on the Y-axis:
plot.LDA <- ggplot(data=Test, aes(x=age, y=hypertension)) +
    geom_point(aes(color=stroke)) + labs(title = " Age vs Hypertension") +
    theme_bw()
plot.LDA
```

The higher the age, the higher the occurrences of strokes (more blue dots on the right).
In case of hypertension, the likelihood of strokes is also higher (more blue dots top right).

```{r}
# 1) Compute the LDA model using lda() function
lda <- lda(formula = stroke ~ age + hypertension, data = Test)
lda
```

```{r}
# 2) Compute LDA with linDA() function:
linDA <- linDA(Test[,1:2], Test[,3], prior= c(ratio_0, ratio_1))
linDA

```


xxx end xxx
