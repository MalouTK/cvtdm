coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)
ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For heart_disease:
mod3 <- glm(stroke ~ heart_disease, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(heart_disease), max(heart_disease)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)
ggplot(valid.lr) + geom_point(aes(x=heart_disease,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Heart Disease") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For average glucose level:
mod4 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs4 <- coef(mod4)
x_plot4 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot4 <- plogis(coefs4[1] + coefs4[2] * x_plot4)
ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot4, y=y_plot4), col = "black", data = data.frame(x_plot4,y_plot4)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
rm(mod3)
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = train.lr, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(train.lr, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)
ggplot(valid.lr) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = train.lr, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)
ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For heart_disease:
mod3 <- glm(stroke ~ heart_disease, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(heart_disease), max(heart_disease)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)
ggplot(valid.lr) + geom_point(aes(x=heart_disease,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Heart Disease") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For average glucose level:
mod4 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs4 <- coef(mod4)
x_plot4 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot4 <- plogis(coefs4[1] + coefs4[2] * x_plot4)
ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot4, y=y_plot4), col = "black", data = data.frame(x_plot4,y_plot4)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = train.lr, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(train.lr, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)
ggplot(valid.lr) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = train.lr, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)
ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For heart_disease:
mod3 <- glm(stroke ~ heart_disease, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(heart_disease), max(heart_disease), length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)
ggplot(valid.lr) + geom_point(aes(x=heart_disease,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Heart Disease") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For average glucose level:
mod4 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs4 <- coef(mod4)
x_plot4 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot4 <- plogis(coefs4[1] + coefs4[2] * x_plot4)
ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot4, y=y_plot4), col = "black", data = data.frame(x_plot4,y_plot4)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = train.lr, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(train.lr, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)
ggplot(valid.lr) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + labs(title = " Age vs Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = train.lr, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)
ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + labs(title = " Hypertension vs Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For heart_disease:
mod3 <- glm(stroke ~ heart_disease, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(heart_disease), max(heart_disease), length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)
ggplot(valid.lr) + geom_point(aes(x=heart_disease,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Heart Disease") + ylab("Stroke") + labs(title = " Heart disease vs Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For average glucose level:
mod4 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs4 <- coef(mod4)
x_plot4 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot4 <- plogis(coefs4[1] + coefs4[2] * x_plot4)
ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot4, y=y_plot4), col = "black", data = data.frame(x_plot4,y_plot4)) + xlab("Average glucose") + ylab("Stroke") + labs(title = " Average glucose level vs Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# Confusion matrix with cut-off 0.5:
set.seed(1)
pred.lr = predict(logit.reg, valid.lr[,-10], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.0484 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.06 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.058 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.0575 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.057 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Confusion matrix with best cut-off (0.4):
conf.lr = confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
# Confusion matrix with best cut-off (0.4):
conf.lr = confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
set.seed(1)
Stroke.dummy = Stroke   # Stroke.withNA
# Put work_type as dummy:
work_type_Dummies <- cbind(Stroke.dummy[1:4], dummy(Stroke.dummy$work_type, sep = "_"), Stroke.dummy[6:10])
names(work_type_Dummies)[5:9] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
#work_type_Dummies
Stroke.dummy <- work_type_Dummies
# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke.dummy[1:12], dummy(Stroke.dummy$smoking_status, sep = "_"), Stroke.dummy[14])
names(smoking_status_Dummies)[13:16] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
#smoking_status_Dummies
Stroke.dummy <- smoking_status_Dummies
train.index.ct = sample(c(1:NROW(Stroke.dummy)), round(NROW(Stroke.dummy)*0.5))
train.ct = Stroke.dummy[train.index.ct,]  # Train set
valid.test.ct = Stroke.dummy[-train.index.ct,]
valid.index.ct = sample(c(1:NROW(valid.test.ct)), round(NROW(valid.test.ct)*0.6))
valid.ct = valid.test.ct[valid.index.ct,] # Valid set
test.ct = valid.test.ct[-valid.index.ct,] # Test set
dim(train.ct)
dim(valid.ct)
dim(test.ct)
# Full-grown tree:
set.seed(1)
c.tree = rpart(stroke ~., data = train.ct, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5, parms = list(prior= c(ratio_0, ratio_1)))
length(c.tree$frame$var[c.tree$frame$var == "<leaf>"])
c.tree$variable.importance
prp(c.tree)
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# error: predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
cp.plot = ggplot(data= metrics.ct) + geom_line(aes(x=row.cp,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=row.cp,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.001 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cp row")
cp.plot
metrics.ct$difference = sqrt((metrics.ct$accuracy - metrics.ct$sensitivity)^2)
row.cp.best = row.cp[which.min(metrics.ct$difference)]
cp.best = c.tree$cptable[,"CP"][row.cp][row.cp.best]
cp.best
set.seed(1)
best.pruned <- prune(c.tree, cp = cp.best)
pred.best.pruned <- predict(best.pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
prp(best.pruned)
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
cp.plot = ggplot(data= metrics.ct) + geom_line(aes(x=row.cp,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=row.cp,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.001 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cp row")
cp.plot
metrics.ct$difference = sqrt((metrics.ct$accuracy - metrics.ct$sensitivity)^2)
row.cp.best = row.cp[which.min(metrics.ct$difference)]
cp.best = c.tree$cptable[,"CP"][row.cp][row.cp.best]
cp.best
set.seed(1)
best.pruned <- prune(c.tree, cp = cp.best)
pred.best.pruned <- predict(best.pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
prp(best.pruned)
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
cp.plot = ggplot(data= metrics.ct) + geom_line(aes(x=row.cp,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=row.cp,y=accuracy, color = "Accuracy")) + theme_classic() + ggtitle("Sensitivity and Accuracy cp row")
cp.plot
metrics.ct$difference = sqrt((metrics.ct$accuracy - metrics.ct$sensitivity)^2)
row.cp.best = row.cp[which.min(metrics.ct$difference)]
cp.best = c.tree$cptable[,"CP"][row.cp][row.cp.best]
cp.best
set.seed(1)
best.pruned <- prune(c.tree, cp = cp.best)
pred.best.pruned <- predict(best.pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
prp(best.pruned)
# Confusion matrix for CT:
conf.ct = confusionMatrix(pred.best.pruned, valid.ct$stroke, positive = "1")
conf.ct
fourfoldplot(confusionMatrix(pred.best.pruned, valid.ct$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
# Dummify:
Stroke$stroke <- as.integer(ifelse(Stroke$stroke == "0", 0,1))  # change stroke to int during dummification
dummies = dummyVars(~ ., data=Stroke)
Stroke.fltr = as.data.frame(predict(dummies, newdata = Stroke)[,-10])
Stroke.fltr$stroke = as.factor(Stroke.fltr$stroke)  # change stroke back to factor
str(Stroke.fltr)
prop.table(table(Stroke.fltr$stroke))
set.seed(1)
train.index.knn = sample(NROW(Stroke.fltr), round(NROW(Stroke.fltr)*0.6))
train.knn = Stroke.fltr[ train.index.knn,]
valid.knn  = Stroke.fltr[-train.index.knn,]
train.index.knn = sample(c(1:NROW(Stroke.fltr)), round(NROW(Stroke.fltr)*0.5))
train.knn = Stroke.fltr[train.index.knn,]  # Train set
valid.test.knn = Stroke.fltr[-train.index.knn,]
valid.index.knn = sample(c(1:NROW(valid.test.knn)), round(NROW(valid.test.knn)*0.6))
valid.knn = valid.test.knn[valid.index.knn,] # Valid set
test.knn = valid.test.knn[-valid.index.knn,] # Test set
dim(train.knn)
dim(valid.knn)
dim(test.knn)
print(table(train.knn$stroke) / table(Stroke.fltr$stroke)) #Training set partition
print(table(valid.knn$stroke) / table(Stroke.fltr$stroke)) #Validation set partition
print(table(test.knn$stroke) / table(Stroke.fltr$stroke)) #Validation set partition
# Normalize the dummified sets:
train.norm.knn = train.knn
valid.norm.knn = valid.knn
test.norm.knn = test.knn
# Normalization of numerical variables:
norm.values = preProcess(train.knn[, c(3,12,13)], method=c("range"))
train.norm.knn[, c(3,12,13)] = predict(norm.values, train.knn[, c(3,12,13)])
valid.norm.knn[, c(3,12,13)] = predict(norm.values, valid.knn[, c(3,12,13)])
test.norm.knn[, c(3,12,13)] = predict(norm.values, test.knn[, c(3,12,13)])
### Creating the graph for best K. (column 21/22 = stroke0/stroke1)
set.seed(1)
nk = 10
metrics.knn <- data.frame(k = seq(1, nk, 1), accuracy = rep(0, nk), sensitivity = rep(0, nk), F1_score = rep(0, nk))
for(i in 1:nk) {
knn.pred = knn(train.norm.knn[, -18], valid.norm.knn[, -18], cl = train.norm.knn[, 18], k = i)
metrics.knn[i, 2] = confusionMatrix(knn.pred, valid.norm.knn[, 18], positive = "1")$overall[1]
metrics.knn[i, 3] = confusionMatrix(knn.pred, valid.norm.knn[, 18], positive = "1")$byClass[1]
}
accuracy.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=accuracy), color="darkred") + theme_classic()
sensitivity.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=sensitivity), color="blue") + theme_classic()
both.plot = ggplot(data= metrics.knn) + geom_line(aes(x=k,y=accuracy), color="darkred") + geom_line(aes(x=k,y=sensitivity))+ ylab("Both") + theme_classic()
plot_grid(accuracy.plot, sensitivity.plot, both.plot, ncol = 1, align = "v")
# Compute best trade-off sensitivity-accuracy
metrics.knn$difference = sqrt((metrics.knn$accuracy - metrics.knn$sensitivity)^2)
k.best = metrics.knn$k[which.min(metrics.knn$difference)]
k.best
### Creating the confusion Matrix --> need just to change the K = 1 & the 21 corresponds to the stroke variable.
pred.knn = knn(train.norm.knn[, -18], valid.norm.knn[, -18], cl = train.norm.knn[, 18], k = k.best, prob = T)
## k.best = 1:
conf.knn = confusionMatrix(pred.knn, valid.norm.knn[, 18], positive = "1")
conf.knn
fourfoldplot(confusionMatrix(pred.knn, valid.norm.knn[, 18], positive = "1")$table, color = c("#CC6666", "#99CC99"))
### Creating the confusion Matrix --> need just to change the K = 1 & the 21 corresponds to the stroke variable.
pred.knn = knn(train.norm.knn[, -18], valid.norm.knn[, -18], cl = train.norm.knn[, 18], k = k.best, prob = T)
## k.best = 1:
conf.knn = confusionMatrix(pred.knn, valid.norm.knn[, 18], positive = "1")
conf.knn
fourfoldplot(confusionMatrix(pred.knn, valid.norm.knn[, 18], positive = "1")$table, color = c("#CC6666", "#99CC99"))
pred.ct.prob = predict(best.pruned, valid.ct, type = "prob", parms = list(prior= c(ratio_0, ratio_1)))[,2]
pred.knn.prob = attr(pred.knn, "prob")
# LR:
results.df = data.frame(actual = valid.lr$stroke, lr.binary = as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), lr.prob = pred.lr, ct.binary = pred.ct, ct.prob = pred.ct.prob, knn.binary = pred.knn, knn.prob = pred.knn.prob)
for(i in 1:NROW(results.df)){
fix_prob = if(results.df[i,"knn.binary"]==1) results.df[i,"knn.prob"] else 1 - results.df[i,"knn.prob"]
results.df[i,"knn.prob"] = fix_prob
}
accuracy.df = data.frame("LR.Accuracy" = conf.lr$overall[1], "CT.Accuracy" = conf.ct$overall[1], "KNN.Accuracy" = conf.knn$overall[1])
sensitivity.df = data.frame("LR.Sensitivity" = conf.lr$byClass[1], "CT.Sensitivity" = conf.ct$byClass[1], "KNN.Sensitivity" = conf.knn$byClass[1])
# Ensembles:
results.df$average.prob = apply(results.df[,c(3,5,7)],FUN = mean, MARGIN = 1)
results.df[,c(2,4,6)] = sapply(results.df[,c(2,4,6)], as.integer) - 1
results.df$major.voting = as.factor(apply(results.df[,c(2,4,6)],FUN = function(x)
names(table(melt(x)$value))[which.max(table(melt(x)$value))], MARGIN = 1))
# Majority vote (ensemble):
accuracy.df = cbind(accuracy.df, ensemble = confusionMatrix(results.df$major.voting, results.df$actual, positive = "1")$overall[1])
sensitivity.df = cbind(sensitivity.df, ensemble = confusionMatrix(results.df$major.voting, results.df$actual, positive = "1")$byClass[1])
# Average (ensemble):
# Find the cutoff for average (ensemble): that maximizes both sensitivity and overall accuracy
# Find the best ensemble cut-off:
cut_off.ens = seq(0.01,0.99,0.01)
metrics.ens <- data.frame(cut_off.ens, sensitivity = rep(0, length(cut_off.ens)), accuracy = rep(0, length(cut_off.ens)))
for (i in 1:length(cut_off.ens)){
x <- as.factor(ifelse(results.df$average.prob >= cut_off.ens[i],1,0))
metrics.ens[i, 2] <- confusionMatrix(x, results.df$actual, positive = "1")$byClass[1]
metrics.ens[i, 3] <- confusionMatrix(x, results.df$actual, positive = "1")$overall[1]
}
cutoff.plot.ens = ggplot(data= metrics.ens) + geom_line(aes(x=cut_off.ens,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off.ens,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.019 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot.ens
metrics.ens$difference = sqrt((metrics.ens$accuracy - metrics.ens$sensitivity)^2)
cut_off.best.ens = cut_off.ens[which.min(metrics.ens$difference)]
cut_off.best.ens
accuracy.df = cbind(accuracy.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$overall[1])
sensitivity.df = cbind(sensitivity.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$byClass[1])
# Average (ensemble):
# Find the cutoff for average (ensemble): that maximizes both sensitivity and overall accuracy
# Find the best ensemble cut-off:
cut_off.ens = seq(0.01,0.99,0.01)
metrics.ens <- data.frame(cut_off.ens, sensitivity = rep(0, length(cut_off.ens)), accuracy = rep(0, length(cut_off.ens)))
for (i in 1:length(cut_off.ens)){
x <- as.factor(ifelse(results.df$average.prob >= cut_off.ens[i],1,0))
metrics.ens[i, 2] <- confusionMatrix(x, results.df$actual, positive = "1")$byClass[1]
metrics.ens[i, 3] <- confusionMatrix(x, results.df$actual, positive = "1")$overall[1]
}
cutoff.plot.ens = ggplot(data= metrics.ens) + geom_line(aes(x=cut_off.ens,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off.ens,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.03 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot.ens
metrics.ens$difference = sqrt((metrics.ens$accuracy - metrics.ens$sensitivity)^2)
cut_off.best.ens = cut_off.ens[which.min(metrics.ens$difference)]
cut_off.best.ens
accuracy.df = cbind(accuracy.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$overall[1])
sensitivity.df = cbind(sensitivity.df, ensemble.prob = confusionMatrix(as.factor(ifelse(results.df$average.prob > cut_off.best.ens, 1, 0)), results.df$actual, positive = "1")$byClass[1])
accuracy.df.melt = melt(accuracy.df)
accuracy.plot = ggplot(aes(x=reorder(variable, value), y=value), data = accuracy.df.melt) + geom_col(fill="darkblue") + ylab("accuracy") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
accuracy.plot
accuracy.df.melt = melt(accuracy.df)
accuracy.plot = ggplot(aes(x=reorder(variable, value), y=value), data = accuracy.df.melt) + geom_col(fill="darkblue") + ylab("accuracy") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
accuracy.plot
rm(accuracy.plot)
rm(accuracy.df.melt)
View(accuracy.df)
accuracy.df <- accuracy.df[,1:5]
accuracy.df.melt = melt(accuracy.df)
accuracy.plot = ggplot(aes(x=reorder(variable, value), y=value), data = accuracy.df.melt) + geom_col(fill="darkblue") + ylab("accuracy") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
accuracy.plot
sensitivity.df.melt = melt(sensitivity.df)
sensitivity.plot = ggplot(aes(x=reorder(variable, value), y=value), data = sensitivity.df.melt) + geom_col(fill="darkblue") + ylab("sensitivity") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
sensitivity.plot
sensitivity.df <- sensitivity.df[,1:5]
accuracy.df.melt = melt(accuracy.df)
accuracy.plot = ggplot(aes(x=reorder(variable, value), y=value), data = accuracy.df.melt) + geom_col(fill="darkblue") + ylab("accuracy") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
accuracy.plot
sensitivity.df.melt = melt(sensitivity.df)
sensitivity.plot = ggplot(aes(x=reorder(variable, value), y=value), data = sensitivity.df.melt) + geom_col(fill="darkblue") + ylab("sensitivity") + xlab("model") +
geom_text(aes(label = round(value, 4)), vjust = -0.5) + theme_classic()
sensitivity.plot
both.melt = accuracy.df.melt
both.melt[,1] = c("LR", "CT", "KNN", "Ensemble", "Ensemble.prob")
both.melt[,3] <- sensitivity.df.melt$value
colnames(both.melt) <- c("method", "accuracy", "sensitivity")
both.melt.plot = ggplot(both.melt, aes(x = method, y= accuracy)) + geom_col(aes(fill = "Accuracy"), width = 0.45, position = position_nudge(x = -0.225)) + geom_text(aes(label = round(accuracy,3)), colour = "grey38", size = 3.5, vjust = -0.3, position = position_nudge(x = -0.225))+
geom_col(aes(y = sensitivity, fill = "Sensitivity"), width = 0.45, position = position_nudge(x = 0.225)) + geom_text(aes(label = round(sensitivity,3)), colour = "grey38", size = 3.5, vjust = -0.3, position = position_nudge(x = 0.225))+
ylab("Model performance") + xlab("model") + ggtitle("Models comparison") + scale_fill_manual(values = c("lightskyblue", "mediumvioletred")) + theme_classic()
both.melt.plot
acc = c(accuracy.df[1,1], accuracy.df[1,2], accuracy.df[1,3], accuracy.df[1,4], accuracy.df[1,5])
sens = c(sensitivity.df[1,1], sensitivity.df[1,2], sensitivity.df[1,3], sensitivity.df[1,4], sensitivity.df[1,5])
method = c("LR", "CT", "KNN", "Ensemble", "Ensemble.prob")
both.df = data.frame(method, "accuracy"= acc, "sensitivity"=sens)
both.df$sum = both.df$accuracy + both.df$sensitivity
#barplot(sum ~ method, both.df, main = "Methods overall comparison", ylab = "accuracy + sensitivity", xlab = "", las = 2)
both.plot = ggplot(aes(x=method, y=sum), data = both.df) + geom_col(fill = "darkgreen") + ylab("sum") + xlab("model") + ggtitle("Methods overall comparison") + geom_text(aes(label = round(sum,4)), colour = "grey38", size = 4, vjust = -0.3) + theme_classic()
both.plot
set.seed(1)
# Reduced GLM
logit.reg2 <- glm(stroke ~ age + hypertension + heart_disease + avg_glucose_level , data = train.lr, family = "binomial")
options(scipen = 999)
summary(logit.reg2)
pred.lr2 = predict(logit.reg2, valid.lr[,-10], type = "response")
# Confusion matrix with best cut-off (0.4):
conf.lr2 = confusionMatrix(as.factor(ifelse(pred.lr2 > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr2
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr2 > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
lr.plots = both.df[1,c(1,4)]
lr.plots[2,1] = c("LR.reduced")
lr.plots[2,2] = conf.lr2$overall[1]+conf.lr2$byClass[1]
lr.plots.plot = ggplot(aes(x=method, y=sum), data = lr.plots) + geom_col(fill = "darkviolet") + ylab("sum") + xlab("model") + ggtitle("LR VS reduced LR") + geom_text(aes(label = round(sum,4)), colour = "grey38", size = 4, vjust = -0.3) + theme_classic()
lr.plots.plot
# LR on test set:
pred.test.lr = predict(logit.reg, test.lr[,-10], type = "response")
conf.test.lr = confusionMatrix(as.factor(ifelse(pred.test.lr > cut_off.best, 1, 0)), test.lr$stroke, positive = "1")
conf.test.lr
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.test.lr > cut_off.best, 1, 0)), test.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
# Reduced LR on test set:
pred.test.lr2 = predict(logit.reg2, test.lr[,-10], type = "response")
conf.test.lr2 = confusionMatrix(as.factor(ifelse(pred.test.lr2 > cut_off.best, 1, 0)), test.lr$stroke, positive = "1")
conf.test.lr2
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.test.lr2 > cut_off.best, 1, 0)), test.lr$stroke, positive = "1")$table, color = c("#CC6666", "#99CC99"))
lr.plots[1,3] = conf.test.lr$overall[1]+conf.test.lr$byClass[1]
lr.plots[2,3] = conf.test.lr2$overall[1]+conf.test.lr2$byClass[1]
colnames(lr.plots) <- c("method", "valid", "test")
lr.test.plots = ggplot(lr.plots, aes(x = method, y= valid)) + geom_col(aes(fill = "Validation"), width = 0.45, position = position_nudge(x = -0.225)) + geom_text(aes(label = round(valid,4)), colour = "grey38", size = 3.5, vjust = -0.3, position = position_nudge(x = -0.225))+
geom_col(aes(y = test, fill = "Test"), width = 0.45, position = position_nudge(x = 0.225)) + geom_text(aes(label = round(test,4)), colour = "grey38", size = 3.5, vjust = -0.3, position = position_nudge(x = 0.225))+
ylab("Accuracy Sensitivity mix") + xlab("model") + ggtitle("Best models in Validation & Test set") + theme_classic()
lr.test.plots
outcome = data.frame(actual = valid.lr$stroke, prob = pred.lr2)
lift <- lift(actual ~ prob, data = outcome)
gain <- gains(predicted = outcome$prob, actual = as.numeric(as.character(outcome$actual)), groups=dim(outcome)[1])
gain.decile <- gains(predicted = outcome$prob, actual = as.numeric(as.character(outcome$actual)))
#par(mfrow=c(1,2))
plot(c(0, gain$cume.pct.of.total*sum(as.numeric(as.character(outcome$actual)))) ~ c(0, gain$cume.obs),
xlab = "# cases", ylab = "Cumulative", type="l")
lines(c(0,sum(as.numeric(as.character(outcome$actual))))~c(0,dim(outcome)[1]), col="gray", lty=2)
barplot(gain.decile$mean.resp / mean(as.numeric(as.character(outcome$actual))), names.arg = gain.decile$depth, xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
outcome = data.frame(actual = valid.lr$stroke, prob = pred.lr2)
lift <- lift(actual ~ prob, data = outcome)
gain <- gains(predicted = outcome$prob, actual = as.numeric(as.character(outcome$actual)), groups=dim(outcome)[1])
gain.decile <- gains(predicted = outcome$prob, actual = as.numeric(as.character(outcome$actual)))
par(mfrow=c(1,2))
plot(c(0, gain$cume.pct.of.total*sum(as.numeric(as.character(outcome$actual)))) ~ c(0, gain$cume.obs),
xlab = "# cases", ylab = "Cumulative", type="l")
lines(c(0,sum(as.numeric(as.character(outcome$actual))))~c(0,dim(outcome)[1]), col="gray", lty=2)
barplot(gain.decile$mean.resp / mean(as.numeric(as.character(outcome$actual))), names.arg = gain.decile$depth, xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
# ROC curve
ROC <- roc(outcome$actual, outcome$prob)
plot.roc(ROC)
# Compute AUC
auc(ROC)
# Draw a colored scatterplot of age and avg_glucose_level:
plot.LDA <- ggplot(data=valid.lr, aes(x=age, y=hypertension)) +
geom_point(aes(color=stroke)) + labs(title = " Age vs Hypertension") + scale_color_manual(values = c("lightblue", "red")) + theme_bw()
plot.LDA
set.seed(1)
# 1) Compute the LDA model using lda() function
lda <- lda(formula = stroke ~ age + hypertension, data = valid.lr)
lda
set.seed(1)
# 2) Compute LDA with linDA() function:
linDA <- linDA(valid.lr[,c(2,3)], valid.lr[,10], prior= c(ratio_0, ratio_1))
linDA
# Estimate DA line (book page 300)
# 1) we must calculate the difference between each classification function coefficients
a_age = linDA$functions[2,2] - linDA$functions[2,1]
a_avg_glucose = linDA$functions[3,2] - linDA$functions[3,1]
# 2) compute the value of intercept and slope
intercept.DA = a_age/a_avg_glucose * (lda$means[1,1] + lda$means[2,1])/2 + (lda$means[1,2] + lda$means[2,2])/2
#(mean(system_admin$Experience) + mean(system_admin$Training))
slope.DA = - a_age/a_avg_glucose
plot.LDA <- plot.LDA + geom_abline(intercept = intercept.DA, slope = slope.DA, color = "dark green", linetype = "dashed")
plot.LDA
