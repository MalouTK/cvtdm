prp(c.tree)
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
length(row.cp)
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
nrow(c.tree$cptable)
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)-1){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
View(metrics.ct)
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
c.tree$cptable
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
c.tree$cptable[,"CP"][row.cp][1]
c.tree$cptable[,"CP"][row.cp][22]
c.tree$cptable[,"CP"][row.cp][21]
c.tree$cptable[,"CP"][row.cp][23]
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][1])
prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][22])
prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][23])
prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][22])
nrow(c.tree$cptable)
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
set.seed(1)
Stroke.dummy = Stroke
# Put work_type as dummy:
work_type_Dummies <- cbind(Stroke.dummy[1:5], dummy(Stroke.dummy$work_type, sep = "_"), Stroke.dummy[7:11])
names(work_type_Dummies)[6:10] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
#work_type_Dummies
Stroke.dummy <- work_type_Dummies
# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke.dummy[1:13], dummy(Stroke.dummy$smoking_status, sep = "_"), Stroke.dummy[15])
names(smoking_status_Dummies)[14:17] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
#smoking_status_Dummies
Stroke.dummy <- smoking_status_Dummies
train.index.ct = sample(NROW(Stroke.dummy), round(NROW(Stroke.dummy)*0.6))
train.ct = Stroke.dummy[train.index.ct,]
valid.ct = Stroke.dummy[- train.index.ct,]
# Full-grown tree:
set.seed(1)
c.tree = rpart(stroke ~., data = train.ct, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5, parms = list(prior= c(ratio_0, ratio_1)))
length(c.tree$frame$var[c.tree$frame$var == "<leaf>"])
c.tree$variable.importance
prp(c.tree)
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
nrow(row.cp)
row.cp
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in row.cp){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
alter > ncol( reach_mat )
ncol(c.tree$cptable)
nrow(c.tree$cptable)
c.tree$cptable[,"CP"][row.cp][1]
c.tree$cptable[,"CP"][1]
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
c.tree$cptable[,"CP"][row.cp][i]
c.tree$cptable[,"CP"][i]
c.tree$cptable[,"CP"][row.cp]
c.tree$cptable[,"CP"][row.cp][10]
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
i
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
knitr::opts_chunk$set(echo = TRUE)
#clear all the console
list=ls()
rm(list=ls())
cat("\014")
ls()
Stroke <- read.csv(file.choose())
library(ggplot2)
library(FNN)
library(dummies)
library(e1071)
library (caret)
library(reshape2)
library(dplyr)
library(class)
library(naniar)
require(ellipse)
library(cowplot)
library(rpart)
library(rpart.plot)
library(MASS)
library(DiscriMiner)
# Types of variables:
str(Stroke)
# Changing N/A characters in bmi to real NA:
Stroke = replace_with_na(Stroke, replace = list(bmi = "N/A")) # bmi has NA now
# Change bmi from character to numerical variable:
Stroke$bmi <- as.numeric(Stroke$bmi)
Stroke$stroke <- as.factor(Stroke$stroke)
# Change character variables as categorical (factor)
Stroke[,c(2,6:8,11)] <- lapply(Stroke[,c(2,6:8,11)], as.factor)
# Remove id row
Stroke = Stroke[,-1]
str(Stroke)
# Show how many different values there is for each variable
sapply(Stroke, function(x) length(unique(x)))
summary(Stroke)
table(Stroke$stroke)
ratio_0 = 4861/(4861+249)
ratio_1 = 1 - ratio_0
ratio_0
ratio_1
# Visualize the data:
# gender:
ggplot(data = Stroke, aes(x = gender, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Gender vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# age:
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_boxplot() + ggtitle("Age vs Stroke")
ggplot(data = Stroke, aes(x = age, fill = stroke)) + geom_histogram() + ggtitle("Age vs Stroke")
# hypertension:
ggplot(data = Stroke, aes(x = hypertension, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Hypertension vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# heart disease:
ggplot(data = Stroke, aes(x = heart_disease, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Heart disease vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# ever married:
ggplot(data = Stroke, aes(x = ever_married, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Ever married vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# work type:
ggplot(data = Stroke, aes(x = work_type, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Work type vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# Residence type:
ggplot(data = Stroke, aes(x = Residence_type, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Residence type vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# Average glucose level:
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_boxplot() + ggtitle("Average glucose level vs Stroke")
ggplot(data = Stroke, aes(x = avg_glucose_level, fill = stroke)) + geom_histogram() + ggtitle("Average glucose level vs Stroke")
# BMI:
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_boxplot() + ggtitle("BMI vs Stroke")
ggplot(data = Stroke, aes(x = bmi, fill = stroke)) + geom_histogram() + ggtitle("BMI vs Stroke")
# Smoking status:
ggplot(data = Stroke, aes(x = smoking_status, fill = stroke)) + geom_bar(position="dodge") + ggtitle("Smoking status vs Stroke") + geom_text(aes(label = ..count..), stat = "count", colour = "grey38", size = 3, vjust = -0.3, position = position_dodge(.9))
# Common data standardization
# Remove only row with gender = "Other"
Stroke = Stroke[-3117,]
# Set 2 levels for gender (and not 3):
Stroke$gender <- factor(Stroke$gender, levels = c("Female", "Male"))
# Omit rows with NAs in bmi (since LR and KNN does not consider NAs):
Stroke.withNA = Stroke  # keep a copy with NAs
Stroke <- na.exclude(Stroke)
set.seed(1)
train.lr <- sample(row.names(Stroke), 0.6*dim(Stroke)[1])
valid.lr <- setdiff(row.names(Stroke), train.lr)
train.lr <- Stroke[train.lr, ]
valid.lr <- Stroke[valid.lr, ]
dim(train.lr)
dim(valid.lr)
# The data has been correctly partitioned into 60% (3'065 obs) in T and 40% in V (2'044 obs)
# 1) Logistic Regression
logit.reg <- glm(stroke ~., data = train.lr, family = "binomial")
options(scipen = 999)
summary(logit.reg)
# Graphs for the significant variables
# For age:
mod <- glm(stroke ~ age, data = train.lr, binomial("logit"))
coefs <- coef(mod)
x_plot <- with(train.lr, seq(min(age), max(age)+25, length.out = 200))
y_plot <- plogis(coefs[1] + coefs[2] * x_plot)
ggplot(valid.lr) + geom_point(aes(x=age,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot, y=y_plot), col = "black", data = data.frame(x_plot,y_plot)) + xlab("Age") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For hypertension:
mod2 <- glm(stroke ~ hypertension, data = train.lr, binomial("logit"))
coefs2 <- coef(mod2)
x_plot2 <- with(train.lr, seq(min(hypertension), max(hypertension), length.out = 200))
y_plot2 <- plogis(coefs2[1] + coefs2[2] * x_plot2)
ggplot(valid.lr) + geom_point(aes(x=hypertension,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot2, y=y_plot2), col = "black", data = data.frame(x_plot2,y_plot2)) + xlab("Hypertension") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# For average glucose level:
mod3 <- glm(stroke ~ avg_glucose_level, data = train.lr, binomial("logit"))
coefs3 <- coef(mod3)
x_plot3 <- with(train.lr, seq(min(avg_glucose_level), max(avg_glucose_level)+25, length.out = 200))
y_plot3 <- plogis(coefs3[1] + coefs3[2] * x_plot3)
ggplot(valid.lr) + geom_point(aes(x=avg_glucose_level,y=as.integer(stroke)-1, color=stroke)) + geom_line(aes(x= x_plot3, y=y_plot3), col = "black", data = data.frame(x_plot3,y_plot3)) + xlab("Average glucose") + ylab("Stroke") + scale_x_continuous(breaks = seq(0,250,50)) + theme_classic()
# Confusion matrix with cut-off 0.5:
set.seed(1)
pred.lr = predict(logit.reg, valid.lr[,-18], type = "response")
confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > 0.5, 1, 0)), valid.lr$stroke, positive = "1")$table)
# Find the best cut-off:
cut_off = seq(0.01,0.99,0.01)
metrics.lr <- data.frame(cut_off, sensitivity = rep(0, length(cut_off)), accuracy = rep(0, length(cut_off)))
for (i in 1:length(cut_off)){
x <- as.factor(ifelse(pred.lr >= cut_off[i],1,0))
metrics.lr[i, 2] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$byClass[1]
metrics.lr[i, 3] <- confusionMatrix(x, valid.lr$stroke, positive = "1")$overall[1]
}
cutoff.plot = ggplot(data= metrics.lr) + geom_line(aes(x=cut_off,y=sensitivity, color="Sensitivity")) + geom_line(aes(x=cut_off,y=accuracy, color = "Accuracy")) + geom_vline(xintercept = 0.054 , linetype="dashed") + theme_classic() + ggtitle("Sensitivity and Accuracy cut-off")
cutoff.plot
metrics.lr$difference = sqrt((metrics.lr$accuracy - metrics.lr$sensitivity)^2)
cut_off.best = cut_off[which.min(metrics.lr$difference)]
cut_off.best
# Confusion matrix with best cut-off (0.4):
conf.lr = confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")
conf.lr
fourfoldplot(confusionMatrix(as.factor(ifelse(pred.lr > cut_off.best, 1, 0)), valid.lr$stroke, positive = "1")$table)
set.seed(1)
Stroke.dummy = Stroke
# Put work_type as dummy:
work_type_Dummies <- cbind(Stroke.dummy[1:5], dummy(Stroke.dummy$work_type, sep = "_"), Stroke.dummy[7:11])
names(work_type_Dummies)[6:10] <- c("children","Govt_job", "Never_Worked", "Private", "Self-employed")
#work_type_Dummies
Stroke.dummy <- work_type_Dummies
# Put smoking_status as dummy:
smoking_status_Dummies <- cbind( Stroke.dummy[1:13], dummy(Stroke.dummy$smoking_status, sep = "_"), Stroke.dummy[15])
names(smoking_status_Dummies)[14:17] <- c("formerly_smoked", "never_smoked", "smokes", "unknown")
#smoking_status_Dummies
Stroke.dummy <- smoking_status_Dummies
train.index.ct = sample(NROW(Stroke.dummy), round(NROW(Stroke.dummy)*0.6))
train.ct = Stroke.dummy[train.index.ct,]
valid.ct = Stroke.dummy[- train.index.ct,]
# Full-grown tree:
set.seed(1)
c.tree = rpart(stroke ~., data = train.ct, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5, parms = list(prior= c(ratio_0, ratio_1)))
length(c.tree$frame$var[c.tree$frame$var == "<leaf>"])
c.tree$variable.importance
prp(c.tree)
# Find the best pruned tree:
set.seed(1)
printcp(c.tree)
prune_cp.index = which.min(c.tree$cptable[,"xerror"])
prune_cp = c.tree$cptable[prune_cp.index,"CP"]
prune_cp
p <- plotcp(c.tree)
p + abline(v = prune_cp.index, lty = "dashed")
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][22]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][1]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][22]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
View(valid.ct)
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][1]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][21]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][20]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][1]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][2]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][3]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][4]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][5]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][6]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
c.tree$cptable[,"CP"][row.cp][6]
predict(prune(c.tree, cp = 0.006925182), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][7]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][5]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][22]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
# Pruned tree:
set.seed(1)
pruned.ct <- prune(c.tree, cp = prune_cp, parms = list(prior= c(ratio_0, ratio_1)))
length(pruned.ct$frame$var[pruned.ct$frame$var == "<leaf>"])
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][7]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][5]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][6]), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
c.tree$cptable[,"CP"][row.cp][6]
predict(prune(c.tree, cp = 0.006925181), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.0069), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.006), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.007), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.008), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.007), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.0071), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.0072), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.0075), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.00758), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.0079), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.00793), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
predict(prune(c.tree, cp = 0.00794), valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
# Find the best cp:
pred.ct = predict(c.tree, valid.ct, type = "class")
row.cp = seq(1,nrow(c.tree$cptable),1)
metrics.ct <- data.frame(row.cp, sensitivity = rep(0, length(row.cp)), accuracy = rep(0, length(row.cp)))
for (i in 1:length(row.cp)){
set.seed(1)
pruned <- prune(c.tree, cp = c.tree$cptable[,"CP"][row.cp][i])
x <- predict(pruned, valid.ct, type = "class", parms = list(prior= c(ratio_0, ratio_1)))
metrics.ct[i, 2] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$byClass[1]
metrics.ct[i, 3] <- confusionMatrix(x, valid.ct$stroke, positive = "1")$overall[1]
}
